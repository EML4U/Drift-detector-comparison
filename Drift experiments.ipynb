{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift experiments\n",
    "\n",
    "Executes detectors on different models and saves results in the format:\n",
    "\n",
    "```\n",
    "{'data_id': {'detector_id': {'predictions': [.1,.2,.3],\n",
    "                             'time_detect': 60.00,\n",
    "                             'time_fit': 1.00}}}\n",
    "```\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_50_file /home/eml4u/EML4U/data/amazon/amazon_drift_bow_50.pickle\n",
      "bow_768_file /home/eml4u/EML4U/data/amazon/amazon_drift_bow_768.pickle\n",
      "Samples: 10000 10000 10000\n",
      "Samples: 10000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "# Set data paths\n",
    "config          = yaml.safe_load(open(\"config.yaml\", \"r\"))\n",
    "bow_50_file  = os.path.join(config[\"EMBEDDINGS_DIRECTORY\"], \"amazon_drift_bow_50.pickle\")\n",
    "bow_768_file = os.path.join(config[\"EMBEDDINGS_DIRECTORY\"], \"amazon_drift_bow_768.pickle\")\n",
    "results_file = os.path.join(config[\"EXPERIMENTS_DIRECTORY\"], \"results_a\")\n",
    "print(\"bow_50_file\", bow_50_file)\n",
    "print(\"bow_768_file\", bow_768_file)\n",
    "\n",
    "# Load data\n",
    "data = {}\n",
    "with open(bow_50_file, \"rb\") as handle:\n",
    "    data[\"bow_50\"] = pickle.load(handle)\n",
    "print(\"Samples:\", len(data[\"bow_50\"]['orig'][0]), len(data[\"bow_50\"]['drifted'][0][0]), len(data[\"bow_50\"]['train'][0]))\n",
    "with open(bow_768_file, \"rb\") as handle:\n",
    "    data[\"bow_768\"] = pickle.load(handle)\n",
    "print(\"Samples:\", len(data[\"bow_768\"]['orig'][0]), len(data[\"bow_768\"]['drifted'][0][0]), len(data[\"bow_768\"]['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example data\n",
    "if(False):\n",
    "    print_model = data[\"bow_50\"]\n",
    "    print(type(print_model), len(print_model))\n",
    "    for key, value in print_model.items() :\n",
    "        print (key, type(value), len(value))\n",
    "        for i in range(len(value)) :\n",
    "            print (value[i][0])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "def reduce_dim(data, target_dimensions, pca=None):\n",
    "    \n",
    "    # Recursive call for lists\n",
    "    if(True):\n",
    "        if isinstance(data, list):\n",
    "            if(pca is None):\n",
    "                pca = PCA(n_components=target_dimensions)\n",
    "                pca.fit(data[0])\n",
    "\n",
    "            results = []\n",
    "            for item in data:\n",
    "                results.append(reduce_dim(item, target_dimensions, pca=pca))\n",
    "            return results\n",
    "    \n",
    "    # Check if is 2-dimensional numpy array\n",
    "    if not isinstance(data, np.ndarray) or data.ndim != 2:\n",
    "        raise ValueError(type(data))\n",
    "    \n",
    "    if(pca is None):\n",
    "        pca = PCA(n_components=target_dimensions)\n",
    "        pca.fit(data)\n",
    "    return pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed results file to /home/eml4u/EML4U/data/amazon/results_reduction_test\n",
      "Creating key bow_50_reduced\n",
      "orig 10000 50\n",
      "drifted 10000 50\n",
      "train 10000 50\n"
     ]
    }
   ],
   "source": [
    "# Set data paths\n",
    "results_file = os.path.join(config[\"EXPERIMENTS_DIRECTORY\"], \"results_reduction_test\")\n",
    "print(\"Changed results file to\", results_file)\n",
    "\n",
    "# Create data\n",
    "old_key = \"bow_768\"\n",
    "new_key = \"bow_50_reduced\"\n",
    "print(\"Creating key\", new_key)\n",
    "data[new_key] = data[old_key].copy() # copy all\n",
    "\n",
    "for key in data[new_key]:\n",
    "    # print(type(data[new_key][key]))  # tuple -> create new ones\n",
    "    data[new_key][key] = (reduce_dim(data[new_key][key][0], 50), data[new_key][key][0])\n",
    "    if isinstance(data[new_key][key][0], list):\n",
    "        print(key, len(data[new_key][key][0][0]), len(data[new_key][key][0][0][1]))\n",
    "    elif isinstance(data[new_key][key][0], np.ndarray):\n",
    "        print(key, len(data[new_key][key][0]), len(data[new_key][key][0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results\n",
    "if os.path.isfile(results_file):\n",
    "    with open(results_file, \"rb\") as handle:\n",
    "        results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Call fit funtion, if not already in results\n",
    "def default_fit(detector_id, detector, data_id, data, results, force_run):\n",
    "    if(data_id in results and detector_id in results[data_id] and not force_run):\n",
    "        return\n",
    "    \n",
    "    # Reset results\n",
    "    results_detector = {}\n",
    "    \n",
    "    time_begin = time.time()\n",
    "        \n",
    "    detector.fit(data)\n",
    "    \n",
    "    results_detector[\"time_fit\"] = time.time() - time_begin\n",
    "    \n",
    "    if(data_id not in results):\n",
    "        results[data_id] = {}\n",
    "    results[data_id][detector_id] = results_detector\n",
    "\n",
    "# Compute predictions, if not already in results\n",
    "def default_detect(detector_id, detector, data_id, data, results, force_run):\n",
    "    if(data_id in results and detector_id in results[data_id] and\n",
    "       \"predictions\" in results[data_id][detector_id] and not force_run):\n",
    "        return\n",
    "    \n",
    "    # Get previous results\n",
    "    if(data_id in results and detector_id in results[data_id]):\n",
    "        results_detector = results[data_id][detector_id]\n",
    "    else:\n",
    "        results_detector = {}\n",
    "    \n",
    "    time_begin = time.time()\n",
    "    \n",
    "    results_detector[\"predictions\"] = []\n",
    "    print(data_id, detector_id, end=\" \")\n",
    "    for p in data:\n",
    "        results_detector[\"predictions\"].append(detector.predict_proba(p))\n",
    "        print(len(p) , end=\" \")\n",
    "    print()\n",
    "\n",
    "    results_detector[\"time_detect\"] = time.time() - time_begin\n",
    "\n",
    "    if(data_id not in results):\n",
    "        results[data_id] = {}\n",
    "    results[data_id][detector_id] = results_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.AlibiKSDetector import AlibiKSDetector\n",
    "detector_id = \"AlibiKSDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = AlibiKSDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50_reduced\"\n",
    "detector = AlibiKSDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = AlibiKSDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.AlibiMMDDetector import AlibiMMDDetector\n",
    "detector_id = \"AlibiMMDDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = AlibiMMDDetector(backend = 'pytorch')\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = AlibiMMDDetector(backend = 'pytorch')\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.CosineDetector import CosineSimilarityDriftDetector\n",
    "detector_id = \"CosineDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = CosineSimilarityDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = CosineSimilarityDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.FCITDetector import FCITDriftDetector\n",
    "detector_id = \"FCITDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = FCITDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = FCITDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.KernelTwoSampleDetector import KernelTwoSampleDriftDetector\n",
    "detector_id = \"KernelTwoSampleDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = KernelTwoSampleDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = KernelTwoSampleDriftDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.AlibiLSDD import AlibiLSDDDetector\n",
    "detector_id = \"AlibiLSDDDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = AlibiLSDDDetector(backend='pytorch')\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50_reduced\"\n",
    "detector = AlibiLSDDDetector(backend='pytorch')\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = AlibiLSDDDetector(backend='pytorch')\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.AlibiChiSquaredDetector import AlibiChiSquaredDetector\n",
    "detector_id = \"AlibiChiSquaredDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = AlibiChiSquaredDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = AlibiChiSquaredDetector()\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.AlibiChiSquaredDetector import AlibiChiSquaredDetector\n",
    "detector_id = \"AlibiChiSquaredDetector-FDR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "detector = AlibiChiSquaredDetector(correction = \"fdr\")\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_768\"\n",
    "detector = AlibiChiSquaredDetector(correction = \"fdr\")\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, False)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectors.CDBDDetector import CDBDDetector\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "detector_id = \"CDBDDetector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = \"bow_50\"\n",
    "\n",
    "features = data[data_id]['train'][0]\n",
    "targets = np.array(data[data_id]['train'][1])[:,1] # take the labels from dictionary, convert to np.array and slice to only get the scores\n",
    "targets = targets.astype('int')\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.33, shuffle=False)\n",
    "model = SVC(kernel='linear', random_state=42) # SVM model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "detector = CDBDDetector(model)\n",
    "default_fit   (detector_id, detector, data_id, data[data_id]['orig'][0],    results, True)\n",
    "default_detect(detector_id, detector, data_id, data[data_id]['drifted'][0], results, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(results_file, \"wb\") as handle:\n",
    "    pickle.dump(results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print runtimes\n",
    "if(True):\n",
    "    print(\"Runtimes (fit and detect) in minutes:\")\n",
    "    from pprint import pprint\n",
    "    for data_id in results:\n",
    "        times = {}\n",
    "        for detector_id in results[data_id]:\n",
    "            time = 0\n",
    "            for key in results[data_id][detector_id]:\n",
    "                if(key == \"time_detect\" or key == \"time_fit\"):\n",
    "                    time += results[data_id][detector_id][key]\n",
    "            times[detector_id] = time/60\n",
    "        pprint(sorted(times.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_50 AlibiKSDetector\n",
      " sum p-values: 4.1668124\n",
      " [0.5108 0.4079 0.3725 0.3364] ... [0.5108 0.4079 0.3725 0.3364]\n",
      "bow_50_reduced AlibiKSDetector\n",
      " sum p-values: 6.132953\n",
      " [0.509  0.4831 0.5102 0.4493] ... [0.509  0.4831 0.5102 0.4493]\n",
      "bow_768 AlibiKSDetector\n",
      " sum p-values: 6.753817\n",
      " [0.5101 0.4989 0.4815 0.4608] ... [0.5101 0.4989 0.4815 0.4608]\n",
      "\n",
      "bow_50 AlibiLSDDDetector\n",
      " sum p-values: 1.4399999678134918\n",
      " [0.96 0.48 0.   0.  ] ... [0.96 0.48 0.   0.  ]\n",
      "bow_50_reduced AlibiLSDDDetector\n",
      " sum p-values: 1.4700000137090683\n",
      " [0.69 0.18 0.23 0.18] ... [0.69 0.18 0.23 0.18]\n",
      "bow_768 AlibiLSDDDetector\n",
      " sum p-values: 2.5599999595433474\n",
      " [0.59 0.43 0.31 0.29] ... [0.59 0.43 0.31 0.29]\n"
     ]
    }
   ],
   "source": [
    "# Print results to compare models\n",
    "\n",
    "def print_results(detector_id):\n",
    "    for data_id in results:\n",
    "        print(data_id, detector_id)\n",
    "        print(\" sum p-values:\", np.sum(results[data_id][detector_id][\"predictions\"]))\n",
    "        first = np.round(results[data_id][detector_id][\"predictions\"][:4], 4)\n",
    "        last = np.round(results[data_id][detector_id][\"predictions\"][0:4], 4)\n",
    "        print(\"\", first, \"...\", last)\n",
    "\n",
    "if(True):\n",
    "    print_results(\"AlibiKSDetector\")\n",
    "    print()\n",
    "    print_results(\"AlibiLSDDDetector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

